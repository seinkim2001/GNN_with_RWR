{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/seinkim/bigdas/code/gnn_with_rwr_centrality\")  # 또는 프로젝트 루트 경로로 직접 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from data.load_dataset import load_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# ✅ 디코더 (dot product)\n",
    "def decode(z, edge_index):\n",
    "    return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# ✅ 그래프 및 라벨 로딩\n",
    "G, id2idx = load_graph()\n",
    "data = from_networkx(G)\n",
    "data.train_mask = data.val_mask = data.test_mask = None\n",
    "\n",
    "# ✅ 링크 예측용 데이터 분할\n",
    "transform = RandomLinkSplit(is_undirected=True, add_negative_train_samples=True)\n",
    "train_data, val_data, test_data = transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_dir = \"/Users/seinkim/bigdas/code/gnn_with_rwr_centrality/attributes/generated_50_cora\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 50개의 속성 조합에 대해 Link Prediction 실험을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:40<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = sorted([f for f in os.listdir(attr_dir) if f.endswith(\".npy\")])\n",
    "\n",
    "print(f\"총 {len(files)}개의 속성 조합에 대해 Link Prediction 실험을 시작합니다...\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ✅ 모든 속성 조합에 대해 실험 반복\n",
    "for fname in tqdm(files):\n",
    "    attr = np.load(os.path.join(attr_dir, fname))\n",
    "    x = torch.tensor(attr, dtype=torch.float)\n",
    "\n",
    "    # 속성 적용\n",
    "    train_data.x = x.to(device)\n",
    "    val_data.x = x.to(device)\n",
    "    test_data.x = x.to(device)\n",
    "\n",
    "    # ✅ 모델 선언\n",
    "    model = GCNEncoder(input_dim=x.size(1), hidden_dim=64).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # 학습\n",
    "    model.train()\n",
    "    for epoch in range(201):\n",
    "        optimizer.zero_grad()\n",
    "        z = model(train_data.x, train_data.edge_index)\n",
    "        score = decode(z, train_data.edge_label_index)\n",
    "        labels = train_data.edge_label.to(device).float()\n",
    "        # print(train_data.x.shape, train_data.edge_index.shape, train_data.edge_label, labels.shape)\n",
    "        loss = F.binary_cross_entropy_with_logits(score, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # 평가\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(test_data.x, test_data.edge_index)\n",
    "        # test_score = decode(z, test_data.edge_label_index).sigmoid().cpu().numpy()\n",
    "        test_score = decode(z, test_data.edge_label_index).cpu().numpy()\n",
    "\n",
    "        test_labels = test_data.edge_label.cpu().numpy()\n",
    "\n",
    "        auc = roc_auc_score(test_labels, test_score)\n",
    "        ap = average_precision_score(test_labels, test_score)\n",
    "\n",
    "\n",
    "    # ✅ 결과 저장\n",
    "    results.append({\n",
    "        \"Attribute File\": fname,\n",
    "        \"ROC AUC\": round(auc, 4),\n",
    "        \"Average Precision\": round(ap, 4)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'edge_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape, \u001b[43medge_index\u001b[49m\u001b[38;5;241m.\u001b[39mshape, train_data\u001b[38;5;241m.\u001b[39medge_label_index\u001b[38;5;241m.\u001b[39mshape, train_data\u001b[38;5;241m.\u001b[39medge_label\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'edge_index' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 실험 완료! 결과는 results/link_prediction_generated_sigmoid_test.csv 에 저장됨.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ✅ CSV 저장\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"./results/link_prediction_generated_sigmoid_test.csv\", index=False)\n",
    "print(\"모든 실험 완료! 결과는 results/link_prediction_generated_sigmoid_test.csv 에 저장됨.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Attribute File  ROC AUC  Average Precision\n",
      "0                        attr_adasim_top1.npy   0.8493             0.8521\n",
      "1                       attr_adasim_top10.npy   0.8114             0.8178\n",
      "2                        attr_adasim_top2.npy   0.8470             0.8443\n",
      "3                        attr_adasim_top3.npy   0.8283             0.8311\n",
      "4                        attr_adasim_top4.npy   0.8292             0.8292\n",
      "5                        attr_adasim_top5.npy   0.8130             0.8219\n",
      "6                        attr_adasim_top6.npy   0.8174             0.8176\n",
      "7                        attr_adasim_top7.npy   0.8103             0.8160\n",
      "8                        attr_adasim_top8.npy   0.8049             0.8108\n",
      "9                        attr_adasim_top9.npy   0.8058             0.8107\n",
      "10                      attr_jaccard_top1.npy   0.8521             0.8571\n",
      "11                     attr_jaccard_top10.npy   0.8406             0.8497\n",
      "12                      attr_jaccard_top2.npy   0.8366             0.8358\n",
      "13                      attr_jaccard_top3.npy   0.8537             0.8548\n",
      "14                      attr_jaccard_top4.npy   0.8453             0.8435\n",
      "15                      attr_jaccard_top5.npy   0.8383             0.8456\n",
      "16                      attr_jaccard_top6.npy   0.8375             0.8360\n",
      "17                      attr_jaccard_top7.npy   0.8414             0.8450\n",
      "18                      attr_jaccard_top8.npy   0.8210             0.8293\n",
      "19                      attr_jaccard_top9.npy   0.8311             0.8388\n",
      "20   attr_rwr_adasim_simrank_jaccard_top1.npy   0.9056             0.8982\n",
      "21  attr_rwr_adasim_simrank_jaccard_top10.npy   0.8346             0.8410\n",
      "22   attr_rwr_adasim_simrank_jaccard_top2.npy   0.8588             0.8454\n",
      "23   attr_rwr_adasim_simrank_jaccard_top3.npy   0.8488             0.8417\n",
      "24   attr_rwr_adasim_simrank_jaccard_top4.npy   0.8366             0.8380\n",
      "25   attr_rwr_adasim_simrank_jaccard_top5.npy   0.8254             0.8313\n",
      "26   attr_rwr_adasim_simrank_jaccard_top6.npy   0.8249             0.8329\n",
      "27   attr_rwr_adasim_simrank_jaccard_top7.npy   0.8333             0.8321\n",
      "28   attr_rwr_adasim_simrank_jaccard_top8.npy   0.8412             0.8409\n",
      "29   attr_rwr_adasim_simrank_jaccard_top9.npy   0.8475             0.8513\n",
      "30                          attr_rwr_top1.npy   0.7367             0.7532\n",
      "31                         attr_rwr_top10.npy   0.7860             0.7885\n",
      "32                          attr_rwr_top2.npy   0.7591             0.7694\n",
      "33                          attr_rwr_top3.npy   0.7657             0.7804\n",
      "34                          attr_rwr_top4.npy   0.7749             0.7868\n",
      "35                          attr_rwr_top5.npy   0.7880             0.7956\n",
      "36                          attr_rwr_top6.npy   0.7722             0.7740\n",
      "37                          attr_rwr_top7.npy   0.7682             0.7823\n",
      "38                          attr_rwr_top8.npy   0.7993             0.8053\n",
      "39                          attr_rwr_top9.npy   0.8135             0.8182\n",
      "40                      attr_simrank_top1.npy   0.9331             0.9263\n",
      "41                     attr_simrank_top10.npy   0.8858             0.8765\n",
      "42                      attr_simrank_top2.npy   0.9223             0.9096\n",
      "43                      attr_simrank_top3.npy   0.9150             0.9037\n",
      "44                      attr_simrank_top4.npy   0.9081             0.8971\n",
      "45                      attr_simrank_top5.npy   0.9022             0.8920\n",
      "46                      attr_simrank_top6.npy   0.9021             0.8941\n",
      "47                      attr_simrank_top7.npy   0.8979             0.8862\n",
      "48                      attr_simrank_top8.npy   0.8875             0.8768\n",
      "49                      attr_simrank_top9.npy   0.8885             0.8747\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 50개의 속성 조합에 대해 Link Prediction 실험을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:22<00:00,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 실험 완료! 결과는 results/link_prediction_mlp_decoder_citeseer.csv 에 저장됨.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from data.load_dataset import load_graph\n",
    "\n",
    "# ✅ GCN 인코더 정의\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# MLP 기반 디코더 정의 (element-wise 곱 → MLP)\n",
    "class LinkPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=64, num_layers=2, dropout=0.3):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.lins.append(nn.Linear(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(nn.Linear(hidden_channels, hidden_channels))\n",
    "        self.lins.append(nn.Linear(hidden_channels, 1))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        x = x_i * x_j  # element-wise product\n",
    "        for lin in self.lins[:-1]:\n",
    "            x = lin(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return torch.sigmoid(x).squeeze()\n",
    "\n",
    "# ✅ 데이터 로딩\n",
    "G, id2idx = load_graph()\n",
    "data = from_networkx(G)\n",
    "data.train_mask = data.val_mask = data.test_mask = None\n",
    "transform = RandomLinkSplit(is_undirected=True, add_negative_train_samples=True)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "# ✅ 실험 준비\n",
    "attr_dir = \"/Users/seinkim/bigdas/code/gnn_with_rwr_centrality/attributes/generated_50\"\n",
    "files = sorted([f for f in os.listdir(attr_dir) if f.endswith(\".npy\")])\n",
    "print(f\"총 {len(files)}개의 속성 조합에 대해 Link Prediction 실험을 시작합니다...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "results = []\n",
    "\n",
    "# ✅ 모든 속성 조합에 대해 반복\n",
    "for fname in tqdm(files):\n",
    "    attr = np.load(os.path.join(attr_dir, fname))\n",
    "    x = torch.tensor(attr, dtype=torch.float)\n",
    "\n",
    "    train_data.x = val_data.x = test_data.x = x.to(device)\n",
    "\n",
    "    model = GCNEncoder(input_dim=x.size(1), hidden_dim=64).to(device)\n",
    "    predictor = LinkPredictor(in_channels=64, hidden_channels=64).to(device)\n",
    "    optimizer = optim.Adam(list(model.parameters()) + list(predictor.parameters()), lr=0.01)\n",
    "\n",
    "    학습\n",
    "    model.train()\n",
    "    predictor.train()\n",
    "    for epoch in range(201):\n",
    "        optimizer.zero_grad()\n",
    "        z = model(train_data.x, train_data.edge_index)\n",
    "        src, dst = train_data.edge_label_index\n",
    "        score = predictor(z[src], z[dst])\n",
    "        labels = train_data.edge_label.to(device).float()\n",
    "        loss = F.binary_cross_entropy(score, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # # 학습\n",
    "    # model.train()\n",
    "    # predictor.train()\n",
    "    # for epoch in range(201):\n",
    "    #     optimizer.zero_grad()\n",
    "    #     z = model(train_data.x, train_data.edge_index)\n",
    "\n",
    "    #     src, dst = train_data.edge_label_index\n",
    "    #     score = predictor(z[src], z[dst])  # 예측된 확률\n",
    "    #     labels = train_data.edge_label.to(device).float()  # 실제 라벨 (0 또는 1) \n",
    "\n",
    "    #     # # ✅ 추가: score 및 labels 확인 (처음 한 번만)\n",
    "    #     # if epoch == 0:\n",
    "    #     #     print(f\"score shape: {score.shape}, 예시 값: {score[:5].detach().cpu().numpy()}\")\n",
    "    #     #     print(f\"labels shape: {labels.shape}, 예시 값: {labels[:5].detach().cpu().numpy()}\")\n",
    "\n",
    "    #     loss = F.binary_cross_entropy(score, labels)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "\n",
    "\n",
    "    # 평가\n",
    "    model.eval()\n",
    "    predictor.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(test_data.x, test_data.edge_index)\n",
    "        src, dst = test_data.edge_label_index\n",
    "        test_score = predictor(z[src], z[dst]).cpu().numpy()\n",
    "        test_labels = test_data.edge_label.cpu().numpy()\n",
    "        auc = roc_auc_score(test_labels, test_score)\n",
    "        ap = average_precision_score(test_labels, test_score)\n",
    "\n",
    "    results.append({\n",
    "        \"Attribute File\": fname,\n",
    "        \"ROC AUC\": round(auc, 4),\n",
    "        \"Average Precision\": round(ap, 4)\n",
    "    })\n",
    "\n",
    "# ✅ 결과 저장\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"./results/link_prediction_mlp_decoder.csv\", index=False)\n",
    "print(\"모든 실험 완료! 결과는 results/link_prediction_mlp_decoder.csv 에 저장됨.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8645, 0.9625, 0.2859,  ..., 0.3719, 0.2832, 0.3155],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8645, 0.9625, 0.2859, 0.2865, 0.3269, 0.3617, 0.5772, 0.2792, 0.2663,\n",
      "        0.2670, 0.3145, 0.2799, 0.3848, 0.6394, 0.3652, 0.3074, 0.4501, 0.8342,\n",
      "        0.3686, 0.6848, 0.9841, 0.2777, 0.3368, 0.8823, 0.9572, 0.5807, 0.6041,\n",
      "        0.8396, 0.3408, 0.7583, 0.4486, 0.9990, 0.2593, 0.2284, 0.7171, 0.3310,\n",
      "        0.2618, 0.4220, 0.9925, 0.9963, 0.3372, 0.9464, 0.9984, 0.8470, 0.6580,\n",
      "        0.2902, 0.9747, 0.2579, 0.5816, 1.0000, 0.3210, 0.2554, 0.5661, 0.2834,\n",
      "        0.3429, 0.3368, 0.9780, 0.7293, 0.9737, 0.7788, 0.9989, 0.5661, 0.2999,\n",
      "        0.2652, 0.2796, 0.9483, 0.9847, 1.0000, 0.2614, 0.3104, 0.2494, 0.2846,\n",
      "        0.3002, 0.2738, 0.9407, 0.5793, 0.9376, 0.2860, 0.2856, 0.2329, 0.6896,\n",
      "        1.0000, 0.4751, 0.9130, 0.6947, 0.9888, 0.9997, 0.4509, 0.9992, 0.9359,\n",
      "        0.2575, 0.6556, 0.9879, 0.8238, 0.3549, 0.9997, 0.5021, 0.7063, 0.9256,\n",
      "        0.2581], grad_fn=<SliceBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(score[:100])\n",
    "print(labels[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1533,  0.2771, -0.3394,  ..., -0.1005,  0.0219,  0.0314],\n",
      "        [-0.3577,  0.6625, -0.8395,  ..., -0.3492,  0.1824,  0.1626],\n",
      "        [ 0.0325, -0.0257,  0.0020,  ...,  0.0337, -0.0312, -0.0210],\n",
      "        ...,\n",
      "        [-0.1475,  0.2774, -0.3938,  ..., -0.1299,  0.0553,  0.0768],\n",
      "        [-0.3010,  0.6704, -0.8064,  ..., -0.3476,  0.1017,  0.1497],\n",
      "        [-0.0240,  0.0711, -0.1247,  ..., -0.0225, -0.0248,  0.0077]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(z[src][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 64])\n"
     ]
    }
   ],
   "source": [
    "print(z.shape) # shape: [num_edges, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 50개의 Citeseer 속성 조합에 대해 Link Prediction 실험을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:40<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 실험 완료! 결과는 results/link_prediction_mlp_decoder_citeseer.csv 에 저장됨.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from data.load_dataset import load_graph  # Citeseer용 load_graph로 되어 있어야 함!\n",
    "\n",
    "# ✅ GCN 인코더 정의\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# ✅ MLP 디코더 정의\n",
    "class LinkPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=64, num_layers=2, dropout=0.3):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.lins.append(nn.Linear(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(nn.Linear(hidden_channels, hidden_channels))\n",
    "        self.lins.append(nn.Linear(hidden_channels, 1))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        x = x_i * x_j\n",
    "        for lin in self.lins[:-1]:\n",
    "            x = lin(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return torch.sigmoid(x).squeeze()\n",
    "\n",
    "# ✅ Citeseer 데이터 로딩\n",
    "G, id2idx = load_graph()  # Citeseer 그래프 불러오기\n",
    "data = from_networkx(G)\n",
    "data.train_mask = data.val_mask = data.test_mask = None\n",
    "\n",
    "transform = RandomLinkSplit(is_undirected=True, add_negative_train_samples=True)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "# ✅ 속성 경로 (Citeseer용)\n",
    "attr_dir = \"/Users/seinkim/bigdas/code/gnn_with_rwr_centrality/attributes/generated_50_citeseer\"\n",
    "files = sorted([f for f in os.listdir(attr_dir) if f.endswith(\".npy\")])\n",
    "print(f\"총 {len(files)}개의 Citeseer 속성 조합에 대해 Link Prediction 실험을 시작합니다...\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "results = []\n",
    "\n",
    "# ✅ 실험 실행\n",
    "for fname in tqdm(files):\n",
    "    attr = np.load(os.path.join(attr_dir, fname))\n",
    "    x = torch.tensor(attr, dtype=torch.float)\n",
    "\n",
    "    train_data.x = val_data.x = test_data.x = x.to(device)\n",
    "\n",
    "    model = GCNEncoder(input_dim=x.size(1), hidden_dim=64).to(device)\n",
    "    predictor = LinkPredictor(in_channels=64).to(device)\n",
    "    optimizer = optim.Adam(list(model.parameters()) + list(predictor.parameters()), lr=0.01)\n",
    "\n",
    "    # ✅ 학습\n",
    "    model.train()\n",
    "    predictor.train()\n",
    "    for epoch in range(201):\n",
    "        optimizer.zero_grad()\n",
    "        z = model(train_data.x, train_data.edge_index)\n",
    "        src, dst = train_data.edge_label_index\n",
    "        score = predictor(z[src], z[dst])\n",
    "        labels = train_data.edge_label.to(device).float()\n",
    "        loss = F.binary_cross_entropy(score, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # ✅ 평가\n",
    "    model.eval()\n",
    "    predictor.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(test_data.x, test_data.edge_index)\n",
    "        src, dst = test_data.edge_label_index\n",
    "        test_score = predictor(z[src], z[dst]).cpu().numpy()\n",
    "        test_labels = test_data.edge_label.cpu().numpy()\n",
    "        auc = roc_auc_score(test_labels, test_score)\n",
    "        ap = average_precision_score(test_labels, test_score)\n",
    "\n",
    "    results.append({\n",
    "        \"Attribute File\": fname,\n",
    "        \"ROC AUC\": round(auc, 4),\n",
    "        \"Average Precision\": round(ap, 4)\n",
    "    })\n",
    "\n",
    "# ✅ 결과 저장\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"./results/link_prediction_mlp_decoder_citeseer.csv\", index=False)\n",
    "print(\"모든 실험 완료! 결과는 results/link_prediction_mlp_decoder_citeseer.csv 에 저장됨.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GCN2Conv\n\u001b[1;32m     12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCora\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 13\u001b[0m path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(osp\u001b[38;5;241m.\u001b[39mdirname(osp\u001b[38;5;241m.\u001b[39mrealpath(\u001b[38;5;18;43m__file__\u001b[39;49m)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, dataset)\n\u001b[1;32m     14\u001b[0m transform \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCompose([T\u001b[38;5;241m.\u001b[39mNormalizeFeatures(), T\u001b[38;5;241m.\u001b[39mGCNNorm(), T\u001b[38;5;241m.\u001b[39mToSparseTensor()])\n\u001b[1;32m     15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Planetoid(path, dataset, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010, Loss: 0.6418, AUC: 0.8562, AP: 0.8561\n",
      "Epoch 020, Loss: 0.3567, AUC: 0.9419, AP: 0.9365\n",
      "Epoch 030, Loss: 0.2286, AUC: 0.9755, AP: 0.9705\n",
      "Epoch 040, Loss: 0.1368, AUC: 0.9901, AP: 0.9883\n",
      "Epoch 050, Loss: 0.0688, AUC: 0.9972, AP: 0.9968\n",
      "Epoch 060, Loss: 0.0310, AUC: 0.9994, AP: 0.9992\n",
      "Epoch 070, Loss: 0.0274, AUC: 0.9992, AP: 0.9990\n",
      "Epoch 080, Loss: 0.0250, AUC: 0.9997, AP: 0.9996\n",
      "Epoch 090, Loss: 0.0111, AUC: 0.9999, AP: 0.9999\n",
      "Epoch 100, Loss: 0.0049, AUC: 1.0000, AP: 1.0000\n",
      "Epoch 110, Loss: 0.0023, AUC: 1.0000, AP: 1.0000\n",
      "Epoch 120, Loss: 0.0012, AUC: 1.0000, AP: 1.0000\n",
      "Epoch 130, Loss: 0.0007, AUC: 1.0000, AP: 1.0000\n",
      "Epoch 140, Loss: 0.0005, AUC: 1.0000, AP: 1.0000\n",
      "Epoch 150, Loss: 0.0004, AUC: 1.0000, AP: 1.0000\n",
      "Epoch 160, Loss: 0.0003, AUC: 1.0000, AP: 1.0000\n",
      "Epoch 170, Loss: 0.0002, AUC: 1.0000, AP: 1.0000\n",
      "Epoch 180, Loss: 0.0002, AUC: 1.0000, AP: 1.0000\n",
      "Epoch 190, Loss: 0.0002, AUC: 1.0000, AP: 1.0000\n",
      "Epoch 200, Loss: 0.0001, AUC: 1.0000, AP: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# GCN-based Link Prediction on Cora Dataset (Custom Edge Features)\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx, negative_sampling\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Set device and seed\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load Cora dataset\n",
    "dataset = Planetoid(root='./data', name='Cora')\n",
    "data = dataset[0].to(device)\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Compute edge features (centrality + similarity)\n",
    "def compute_edge_features(G, edge_list):\n",
    "    centrality = nx.betweenness_centrality(G)\n",
    "    features = []\n",
    "    for u, v in edge_list:\n",
    "        try:\n",
    "            sim = next(nx.jaccard_coefficient(G, [(u, v)]))[2]\n",
    "        except:\n",
    "            sim = 0.0\n",
    "        features.append([(centrality[u] + centrality[v]) / 2, sim])\n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "# Generate edge feature matrix\n",
    "edge_list = data.edge_index.cpu().numpy().T\n",
    "edge_features = compute_edge_features(G, edge_list)\n",
    "edge_attr = torch.tensor(edge_features, dtype=torch.float).to(device)\n",
    "\n",
    "# Normalize edge attributes\n",
    "edge_attr = (edge_attr - edge_attr.min(dim=0)[0]) / (edge_attr.max(dim=0)[0] - edge_attr.min(dim=0)[0] + 1e-15)\n",
    "\n",
    "# GCN Encoder\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# MLP Link Predictor\n",
    "class LinkPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=64):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(in_channels, hidden_channels)\n",
    "        self.lin2 = nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        x = x_i * x_j\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Generate positive and negative edges\n",
    "pos_edge_index = data.edge_index\n",
    "neg_edge_index = negative_sampling(pos_edge_index, num_nodes=data.num_nodes, num_neg_samples=pos_edge_index.size(1))\n",
    "\n",
    "# Model setup\n",
    "model = GCNEncoder(dataset.num_features, 64).to(device)\n",
    "predictor = LinkPredictor(64).to(device)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(predictor.parameters()), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    predictor.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model(data.x, data.edge_index)\n",
    "\n",
    "    pos_pred = predictor(z[pos_edge_index[0]], z[pos_edge_index[1]])\n",
    "    neg_pred = predictor(z[neg_edge_index[0]], z[neg_edge_index[1]])\n",
    "\n",
    "    # pos_pred = predictor(z[src], z[dst]).squeeze()\n",
    "\n",
    "    pos_label = torch.ones(pos_pred.size(0), device=device)\n",
    "    neg_label = torch.zeros(neg_pred.size(0), device=device)\n",
    "\n",
    "    # loss = F.binary_cross_entropy(torch.cat([pos_pred, neg_pred]), torch.cat([pos_label, neg_label]))\n",
    "    loss = F.binary_cross_entropy(\n",
    "        torch.cat([pos_pred, neg_pred]).squeeze(),\n",
    "        torch.cat([pos_label, neg_label])\n",
    ")\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Evaluation\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    predictor.eval()\n",
    "    z = model(data.x, data.edge_index)\n",
    "    pos_pred = predictor(z[pos_edge_index[0]], z[pos_edge_index[1]])\n",
    "    neg_pred = predictor(z[neg_edge_index[0]], z[neg_edge_index[1]])\n",
    "    preds = torch.cat([pos_pred, neg_pred]).cpu()\n",
    "    labels = torch.cat([torch.ones(pos_pred.size(0)), torch.zeros(neg_pred.size(0))])\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    ap = average_precision_score(labels, preds)\n",
    "    return auc, ap\n",
    "\n",
    "# Run training\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    auc, ap = evaluate()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, AUC: {auc:.4f}, AP: {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실험\n",
    "#### Cora, Citeseer 그래프에서 Link Prediction task를 수행하는 실험\n",
    "- Citeseer 데이터셋에 대해 다양한 노드 속성(attribute) 벡터를 불러와서,\n",
    "- GCN 인코더 + MLP 디코더 구조로 학습한 후,\n",
    "- 링크 예측 성능(AUC, AP)을 측정하고 .csv로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 50개의 속성 조합에 대해 Link Prediction 실험을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:23<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 실험 완료! 결과는 results/link_prediction_mlp_decoder_generated_50_cora.csv 에 저장됨.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from data.load_dataset import load_graph\n",
    "\n",
    "# ✅ GCN 인코더 정의\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# MLP 기반 디코더 정의 (element-wise 곱 → MLP)\n",
    "class LinkPredictor(nn.Module):\n",
    "    '''\n",
    "    z: 모든 노드에 대한 임베딩 벡터들을 담고 있는 행렬\n",
    "    만약 그래프에 N개의 노드가 있고, hidden_dim=64이면\n",
    "    → z의 shape은 [N, 64]가 돼 ex) cora dataset의 z shape은 [2708, 64] \n",
    "\n",
    "    z[i] * z[j] 형태의 element-wise 곱을 받아서 MLP로 edge 존재 여부(0~1 확률)를 예측하는 구조\n",
    "    '''\n",
    "    def __init__(self, in_channels, hidden_channels=64, num_layers=2, dropout=0.3):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.lins.append(nn.Linear(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(nn.Linear(hidden_channels, hidden_channels))\n",
    "        self.lins.append(nn.Linear(hidden_channels, 1))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        x = x_i * x_j  # element-wise product\n",
    "        for lin in self.lins[:-1]:\n",
    "            x = lin(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return torch.sigmoid(x).squeeze()\n",
    "\n",
    "# ✅ 데이터 로딩\n",
    "G, id2idx = load_graph()\n",
    "data = from_networkx(G)\n",
    "data.train_mask = data.val_mask = data.test_mask = None\n",
    "transform = RandomLinkSplit(is_undirected=True, add_negative_train_samples=True)\n",
    "train_data, val_data, test_data = transform(data) # train/val/test 분리\n",
    "\n",
    "# ✅ 실험 준비\n",
    "attr_dir = \"/Users/seinkim/bigdas/code/gnn_with_rwr_centrality/attributes/generated_50_cora\"  # 속성 조합이 저장된 디렉토리\n",
    "files = sorted([f for f in os.listdir(attr_dir) if f.endswith(\".npy\")])\n",
    "print(f\"총 {len(files)}개의 속성 조합에 대해 Link Prediction 실험을 시작합니다...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "results = []\n",
    "\n",
    "# ✅ 모든 속성 조합에 대해 반복\n",
    "for fname in tqdm(files):\n",
    "    attr = np.load(os.path.join(attr_dir, fname))\n",
    "    x = torch.tensor(attr, dtype=torch.float)\n",
    "\n",
    "    train_data.x = val_data.x = test_data.x = x.to(device)\n",
    "\n",
    "    model = GCNEncoder(input_dim=x.size(1), hidden_dim=64).to(device)\n",
    "    predictor = LinkPredictor(in_channels=64, hidden_channels=64).to(device)\n",
    "    optimizer = optim.Adam(list(model.parameters()) + list(predictor.parameters()), lr=0.01)\n",
    "\n",
    "    model.train()\n",
    "    predictor.train()\n",
    "    for epoch in range(201):\n",
    "        optimizer.zero_grad()\n",
    "        z = model(train_data.x, train_data.edge_index)\n",
    "        src, dst = train_data.edge_label_index\n",
    "        score = predictor(z[src], z[dst])\n",
    "        labels = train_data.edge_label.to(device).float()\n",
    "        loss = F.binary_cross_entropy(score, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 평가\n",
    "    model.eval()\n",
    "    predictor.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(test_data.x, test_data.edge_index)\n",
    "        src, dst = test_data.edge_label_index\n",
    "        test_score = predictor(z[src], z[dst]).cpu().numpy()\n",
    "        test_labels = test_data.edge_label.cpu().numpy()\n",
    "        auc = roc_auc_score(test_labels, test_score)\n",
    "        ap = average_precision_score(test_labels, test_score)\n",
    "\n",
    "    results.append({\n",
    "        \"Attribute File\": fname,\n",
    "        \"ROC AUC\": round(auc, 4),\n",
    "        \"Average Precision\": round(ap, 4)\n",
    "    })\n",
    "\n",
    "# ✅ 결과 저장\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"./results/link_prediction_mlp_decoder_generated_50_cora.csv\", index=False)\n",
    "print(\"모든 실험 완료! 결과는 results/link_prediction_mlp_decoder_generated_50_cora.csv 에 저장됨.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPython -- An enhanced Interactive Python\n",
      "=========================================\n",
      "\n",
      "IPython offers a fully compatible replacement for the standard Python\n",
      "interpreter, with convenient shell features, special commands, command\n",
      "history mechanism and output results caching.\n",
      "\n",
      "At your system command line, type 'ipython -h' to see the command line\n",
      "options available. This document only describes interactive features.\n",
      "\n",
      "GETTING HELP\n",
      "------------\n",
      "\n",
      "Within IPython you have various way to access help:\n",
      "\n",
      "  ?         -> Introduction and overview of IPython's features (this screen).\n",
      "  object?   -> Details about 'object'.\n",
      "  object??  -> More detailed, verbose information about 'object'.\n",
      "  %quickref -> Quick reference of all IPython specific syntax and magics.\n",
      "  help      -> Access Python's own help system.\n",
      "\n",
      "If you are in terminal IPython you can quit this screen by pressing `q`.\n",
      "\n",
      "\n",
      "MAIN FEATURES\n",
      "-------------\n",
      "\n",
      "* Access to the standard Python help with object docstrings and the Python\n",
      "  manuals. Simply type 'help' (no quotes) to invoke it.\n",
      "\n",
      "* Magic commands: type %magic for information on the magic subsystem.\n",
      "\n",
      "* System command aliases, via the %alias command or the configuration file(s).\n",
      "\n",
      "* Dynamic object information:\n",
      "\n",
      "  Typing ?word or word? prints detailed information about an object. Certain\n",
      "  long strings (code, etc.) get snipped in the center for brevity.\n",
      "\n",
      "  Typing ??word or word?? gives access to the full information without\n",
      "  snipping long strings. Strings that are longer than the screen are printed\n",
      "  through the less pager.\n",
      "\n",
      "  The ?/?? system gives access to the full source code for any object (if\n",
      "  available), shows function prototypes and other useful information.\n",
      "\n",
      "  If you just want to see an object's docstring, type '%pdoc object' (without\n",
      "  quotes, and without % if you have automagic on).\n",
      "\n",
      "* Tab completion in the local namespace:\n",
      "\n",
      "  At any time, hitting tab will complete any available python commands or\n",
      "  variable names, and show you a list of the possible completions if there's\n",
      "  no unambiguous one. It will also complete filenames in the current directory.\n",
      "\n",
      "* Search previous command history in multiple ways:\n",
      "\n",
      "  - Start typing, and then use arrow keys up/down or (Ctrl-p/Ctrl-n) to search\n",
      "    through the history items that match what you've typed so far.\n",
      "\n",
      "  - Hit Ctrl-r: opens a search prompt. Begin typing and the system searches\n",
      "    your history for lines that match what you've typed so far, completing as\n",
      "    much as it can.\n",
      "\n",
      "  - %hist: search history by index.\n",
      "\n",
      "* Persistent command history across sessions.\n",
      "\n",
      "* Logging of input with the ability to save and restore a working session.\n",
      "\n",
      "* System shell with !. Typing !ls will run 'ls' in the current directory.\n",
      "\n",
      "* The reload command does a 'deep' reload of a module: changes made to the\n",
      "  module since you imported will actually be available without having to exit.\n",
      "\n",
      "* Verbose and colored exception traceback printouts. See the magic xmode and\n",
      "  xcolor functions for details (just type %magic).\n",
      "\n",
      "* Input caching system:\n",
      "\n",
      "  IPython offers numbered prompts (In/Out) with input and output caching. All\n",
      "  input is saved and can be retrieved as variables (besides the usual arrow\n",
      "  key recall).\n",
      "\n",
      "  The following GLOBAL variables always exist (so don't overwrite them!):\n",
      "  _i: stores previous input.\n",
      "  _ii: next previous.\n",
      "  _iii: next-next previous.\n",
      "  _ih : a list of all input _ih[n] is the input from line n.\n",
      "\n",
      "  Additionally, global variables named _i<n> are dynamically created (<n>\n",
      "  being the prompt counter), such that _i<n> == _ih[<n>]\n",
      "\n",
      "  For example, what you typed at prompt 14 is available as _i14 and _ih[14].\n",
      "\n",
      "  You can create macros which contain multiple input lines from this history,\n",
      "  for later re-execution, with the %macro function.\n",
      "\n",
      "  The history function %hist allows you to see any part of your input history\n",
      "  by printing a range of the _i variables. Note that inputs which contain\n",
      "  magic functions (%) appear in the history with a prepended comment. This is\n",
      "  because they aren't really valid Python code, so you can't exec them.\n",
      "\n",
      "* Output caching system:\n",
      "\n",
      "  For output that is returned from actions, a system similar to the input\n",
      "  cache exists but using _ instead of _i. Only actions that produce a result\n",
      "  (NOT assignments, for example) are cached. If you are familiar with\n",
      "  Mathematica, IPython's _ variables behave exactly like Mathematica's %\n",
      "  variables.\n",
      "\n",
      "  The following GLOBAL variables always exist (so don't overwrite them!):\n",
      "  _ (one underscore): previous output.\n",
      "  __ (two underscores): next previous.\n",
      "  ___ (three underscores): next-next previous.\n",
      "\n",
      "  Global variables named _<n> are dynamically created (<n> being the prompt\n",
      "  counter), such that the result of output <n> is always available as _<n>.\n",
      "\n",
      "  Finally, a global dictionary named _oh exists with entries for all lines\n",
      "  which generated output.\n",
      "\n",
      "* Directory history:\n",
      "\n",
      "  Your history of visited directories is kept in the global list _dh, and the\n",
      "  magic %cd command can be used to go to any entry in that list.\n",
      "\n",
      "* Auto-parentheses and auto-quotes (adapted from Nathan Gray's LazyPython)\n",
      "\n",
      "  1. Auto-parentheses\n",
      "        \n",
      "     Callable objects (i.e. functions, methods, etc) can be invoked like\n",
      "     this (notice the commas between the arguments)::\n",
      "       \n",
      "         In [1]: callable_ob arg1, arg2, arg3\n",
      "       \n",
      "     and the input will be translated to this::\n",
      "       \n",
      "         callable_ob(arg1, arg2, arg3)\n",
      "       \n",
      "     This feature is off by default (in rare cases it can produce\n",
      "     undesirable side-effects), but you can activate it at the command-line\n",
      "     by starting IPython with `--autocall 1`, set it permanently in your\n",
      "     configuration file, or turn on at runtime with `%autocall 1`.\n",
      "\n",
      "     You can force auto-parentheses by using '/' as the first character\n",
      "     of a line.  For example::\n",
      "       \n",
      "          In [1]: /globals             # becomes 'globals()'\n",
      "       \n",
      "     Note that the '/' MUST be the first character on the line!  This\n",
      "     won't work::\n",
      "       \n",
      "          In [2]: print /globals    # syntax error\n",
      "\n",
      "     In most cases the automatic algorithm should work, so you should\n",
      "     rarely need to explicitly invoke /. One notable exception is if you\n",
      "     are trying to call a function with a list of tuples as arguments (the\n",
      "     parenthesis will confuse IPython)::\n",
      "       \n",
      "          In [1]: zip (1,2,3),(4,5,6)  # won't work\n",
      "       \n",
      "     but this will work::\n",
      "       \n",
      "          In [2]: /zip (1,2,3),(4,5,6)\n",
      "          ------> zip ((1,2,3),(4,5,6))\n",
      "          Out[2]= [(1, 4), (2, 5), (3, 6)]\n",
      "\n",
      "     IPython tells you that it has altered your command line by\n",
      "     displaying the new command line preceded by -->.  e.g.::\n",
      "       \n",
      "          In [18]: callable list\n",
      "          -------> callable (list)\n",
      "\n",
      "  2. Auto-Quoting\n",
      "    \n",
      "     You can force auto-quoting of a function's arguments by using ',' as\n",
      "     the first character of a line.  For example::\n",
      "       \n",
      "          In [1]: ,my_function /home/me   # becomes my_function(\"/home/me\")\n",
      "\n",
      "     If you use ';' instead, the whole argument is quoted as a single\n",
      "     string (while ',' splits on whitespace)::\n",
      "       \n",
      "          In [2]: ,my_function a b c   # becomes my_function(\"a\",\"b\",\"c\")\n",
      "          In [3]: ;my_function a b c   # becomes my_function(\"a b c\")\n",
      "\n",
      "     Note that the ',' MUST be the first character on the line!  This\n",
      "     won't work::\n",
      "       \n",
      "          In [4]: x = ,my_function /home/me    # syntax error\n"
     ]
    }
   ],
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnwithrwr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
